{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.convolutionLayer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride ,padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        \n",
    "        self.convolutionLayer2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "        \n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "    # Other stuff\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output_layer1 = self.convolutionLayer1(x)\n",
    "        output_layer2 = self.convolutionLayer2(output_layer1)\n",
    "        \n",
    "        # Skip Connection\n",
    "        if self.downsample==True:\n",
    "            residual = self.downsample(x)\n",
    "        \n",
    "            output_layer2 += residual\n",
    "        \n",
    "        \n",
    "        # pass through the activation function\n",
    "        output = self.relu(output_layer2)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualNeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResidualNeuralNetwork, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        \n",
    "        # Convolutional layers\n",
    "        \n",
    "        # in_channels = 3 as we have RGB images\n",
    "        # First Convolution Layer with 64 Filters of size 7x7, stride=2 and padding=3\n",
    "        self.convolutionLayer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Max Pooling layer of size 3x3, stride=2 and padding=1\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Accordind to ResNet 34 Layer model we have 3,4,6,3 layers in each block with 64,128,256,512 filters respectively\n",
    "        \n",
    "        # 1st Block of 3 convolution layers of 64 filters of size 3x3\n",
    "        self.layer1 = self.make_newLayer(block,64,layers[0],stride=1)\n",
    "        \n",
    "        # 2nd Block of 4 convolution layers of 128 filters of size 3x3\n",
    "        self.layer2 = self.make_newLayer(block,128,layers[1],stride=2)\n",
    "        \n",
    "        # 3rd Block of 6 convolution layers of 256 filters of size 3x3\n",
    "        self.layer3 = self.make_newLayer(block,256,layers[2],stride=2)\n",
    "        \n",
    "        # 4th Block of 3 convolution layers of 512 filters of size 3x3\n",
    "        self.layer4 = self.make_newLayer(block,512,layers[3],stride=2)\n",
    "        \n",
    "        # Average Pooling Layer\n",
    "        self.average_pool = nn.AvgPool2d(7, stride=1)\n",
    "        \n",
    "        # Fully Connected Layer consisting of 512 neurons\n",
    "        self.FullyConnectedLayer = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def make_newLayer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.inplanes != planes):\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "            \n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        \n",
    "        #update the inplanes\n",
    "        self.inplanes = planes\n",
    "        \n",
    "        #remaining feature maps(blocks)\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Convolutional Layers\n",
    "        output = self.convolutionLayer1(x)\n",
    "        output = self.maxpool(output)\n",
    "        \n",
    "        # 4 Residual Blocks\n",
    "        output = self.layer1(output)\n",
    "        output = self.layer2(output)\n",
    "        output = self.layer3(output)\n",
    "        output = self.layer4(output)\n",
    "        \n",
    "        # Average Pooling Layer\n",
    "        output = self.average_pool(output)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.FullyConnectedLayer(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_accuracy(message, file_path):\n",
    "    with open(file_path, 'a') as file:\n",
    "        file.write(message)\n",
    "        file.write('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 2\n",
    "\n",
    "model = ResidualNeuralNetwork(ResidualBlock, [3, 4, 6, 3], num_classes).to(device)\n",
    "\n",
    "# Use MSE as loss for regression \n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# change learning rate after 7 epochs\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValidationBlock(model):\n",
    "    \n",
    "    PatchList = ['valid0.parquet', 'valid1.parquet']\n",
    "    \n",
    "    totalLoss = 0\n",
    "    numSamples = 0\n",
    "    for patch in PatchList:\n",
    "        valid_set = pd.read_parquet(patch, dtype_backend='pyarrow')\n",
    "        valid_dataset = []\n",
    "        for i in range(32):\n",
    "            valid_dataset.append((torch.tensor(valid_set['X_jet'][i]), valid_set['m'][i]))\n",
    "\n",
    "        batch_size = 16\n",
    "        valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            for images, labels in valid_loader:\n",
    "                \n",
    "                # resize images\n",
    "                resize_transform = transforms.Compose([transforms.Resize((224, 224))])\n",
    "                images = resize_transform(images)\n",
    "                \n",
    "                # move to device\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                \n",
    "                totalLoss += np.log(criterion(outputs, labels).item())*images.size(0)\n",
    "                numSamples += images.size(0)\n",
    "            \n",
    "                \n",
    "    meanLoss = totalLoss/numSamples\n",
    "    print('Validation Loss: {:.4f}'.format(meanLoss))\n",
    "    write_accuracy('Validation Loss: {:.4f}'.format(meanLoss), 'accuracies.txt')\n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def TrainBlock(PatchList, model):\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        batch = 1\n",
    "        runningLoss = 0\n",
    "        numSamples = 0\n",
    "        for patch in PatchList:\n",
    "            df = pd.read_parquet(patch, dtype_backend='pyarrow')\n",
    "            dataset = []\n",
    "            for i in range(32):\n",
    "                dataset.append((torch.tensor(df['X_jet'][i]), df['m'][i]))\n",
    "            \n",
    "            batch_size = 16\n",
    "            train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "            # Train the model\n",
    "            for images, labels in train_loader:\n",
    "                \n",
    "                # resize images\n",
    "                resize_transform = transforms.Compose([transforms.Resize((224, 224))])\n",
    "                arr = [resize_transform(image) for image in images]\n",
    "                \n",
    "                # convert to tensor\n",
    "                images = torch.stack(arr)\n",
    "                \n",
    "                # move to GPU\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs.squeeze(), torch.tensor(labels, dtype=torch.float))\n",
    "                \n",
    "                # Backward and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                print(\"images.size(0) : \", images.size(0))\n",
    "                runningLoss += np.log(loss.item())*images.size(0)\n",
    "                numSamples += images.size(0)\n",
    "                \n",
    "                print(f\"Batch {batch} Loss: {runningLoss}\")\n",
    "                print(f\"Batch {batch} Samples: {numSamples}\")\n",
    "                del images, labels, outputs\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch {batch} Completed\")\n",
    "                batch += 1\n",
    "        \n",
    "        epochLoss = runningLoss/numSamples\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, epochLoss))\n",
    "        write_accuracy('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, epochLoss), 'accuracies.txt')\n",
    "\n",
    "        # Validation\n",
    "        ValidationBlock(model)\n",
    "    \n",
    "    return model\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestBlock(PatchList):\n",
    "    \n",
    "    totalLoss = 0\n",
    "    numSamples = 0\n",
    "    mean_loss = 0\n",
    "    for patch in PatchList:\n",
    "        df = pd.read_parquet(patch, dtype_backend='pyarrow')\n",
    "        dataset = []\n",
    "        for i in range(32):\n",
    "            dataset.append((torch.tensor(df['X_jet'][i]), df['m'][i]))\n",
    "        \n",
    "        batch_size = 16\n",
    "        test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                \n",
    "                # resize images\n",
    "                resize_transform = transforms.Compose([transforms.Resize((224, 224))])\n",
    "                images = resize_transform(images)\n",
    "                \n",
    "                # move to device\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                \n",
    "                totalLoss += np.log(criterion(outputs, labels).item())*images.size(0)\n",
    "                numSamples += images.size(0)\n",
    "                \n",
    "    meanLoss = totalLoss/numSamples\n",
    "    print('Test Loss: {:.4f}'.format(meanLoss))\n",
    "    write_accuracy('Test Loss: {:.4f}'.format(meanLoss), 'accuracies.txt')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.size(0) :  16\n",
      "Batch 1 Loss: 185.2389628335852\n",
      "Batch 1 Samples: 16\n",
      "Epoch [1/2], Batch 1 Completed\n",
      "images.size(0) :  16\n",
      "Batch 2 Loss: 369.7189498474943\n",
      "Batch 2 Samples: 32\n",
      "Epoch [1/2], Batch 2 Completed\n",
      "images.size(0) :  16\n",
      "Batch 3 Loss: 542.4012821666033\n",
      "Batch 3 Samples: 48\n",
      "Epoch [1/2], Batch 3 Completed\n",
      "images.size(0) :  16\n",
      "Batch 4 Loss: 712.1943971500633\n",
      "Batch 4 Samples: 64\n",
      "Epoch [1/2], Batch 4 Completed\n",
      "images.size(0) :  16\n",
      "Batch 5 Loss: 885.6849909754801\n",
      "Batch 5 Samples: 80\n",
      "Epoch [1/2], Batch 5 Completed\n",
      "images.size(0) :  16\n",
      "Batch 6 Loss: 1033.6447909343412\n",
      "Batch 6 Samples: 96\n",
      "Epoch [1/2], Batch 6 Completed\n",
      "images.size(0) :  16\n",
      "Batch 7 Loss: 1185.7282405873796\n",
      "Batch 7 Samples: 112\n",
      "Epoch [1/2], Batch 7 Completed\n",
      "images.size(0) :  16\n",
      "Batch 8 Loss: 1345.1809538968273\n",
      "Batch 8 Samples: 128\n",
      "Epoch [1/2], Batch 8 Completed\n",
      "images.size(0) :  16\n",
      "Batch 9 Loss: 1504.3737545631748\n",
      "Batch 9 Samples: 144\n",
      "Epoch [1/2], Batch 9 Completed\n",
      "images.size(0) :  16\n",
      "Batch 10 Loss: 1671.8720743773752\n",
      "Batch 10 Samples: 160\n",
      "Epoch [1/2], Batch 10 Completed\n",
      "images.size(0) :  16\n",
      "Batch 11 Loss: 1826.3934446240487\n",
      "Batch 11 Samples: 176\n",
      "Epoch [1/2], Batch 11 Completed\n",
      "images.size(0) :  16\n",
      "Batch 12 Loss: 1981.2310684803194\n",
      "Batch 12 Samples: 192\n",
      "Epoch [1/2], Batch 12 Completed\n",
      "images.size(0) :  16\n",
      "Batch 13 Loss: 2136.912730295087\n",
      "Batch 13 Samples: 208\n",
      "Epoch [1/2], Batch 13 Completed\n",
      "images.size(0) :  16\n",
      "Batch 14 Loss: 2288.28083944234\n",
      "Batch 14 Samples: 224\n",
      "Epoch [1/2], Batch 14 Completed\n",
      "images.size(0) :  16\n",
      "Batch 15 Loss: 2441.0557755072637\n",
      "Batch 15 Samples: 240\n",
      "Epoch [1/2], Batch 15 Completed\n",
      "images.size(0) :  16\n",
      "Batch 16 Loss: 2593.7524730965306\n",
      "Batch 16 Samples: 256\n",
      "Epoch [1/2], Batch 16 Completed\n",
      "images.size(0) :  16\n",
      "Batch 17 Loss: 2753.272375877069\n",
      "Batch 17 Samples: 272\n",
      "Epoch [1/2], Batch 17 Completed\n",
      "images.size(0) :  16\n",
      "Batch 18 Loss: 2904.990998024836\n",
      "Batch 18 Samples: 288\n",
      "Epoch [1/2], Batch 18 Completed\n",
      "images.size(0) :  16\n",
      "Batch 19 Loss: 3064.573527037878\n",
      "Batch 19 Samples: 304\n",
      "Epoch [1/2], Batch 19 Completed\n",
      "images.size(0) :  16\n",
      "Batch 20 Loss: 3219.897556113408\n",
      "Batch 20 Samples: 320\n",
      "Epoch [1/2], Batch 20 Completed\n",
      "images.size(0) :  16\n",
      "Batch 21 Loss: 3373.334382880889\n",
      "Batch 21 Samples: 336\n",
      "Epoch [1/2], Batch 21 Completed\n",
      "images.size(0) :  16\n",
      "Batch 22 Loss: 3534.209238873358\n",
      "Batch 22 Samples: 352\n",
      "Epoch [1/2], Batch 22 Completed\n",
      "images.size(0) :  16\n",
      "Batch 23 Loss: 3695.199802597433\n",
      "Batch 23 Samples: 368\n",
      "Epoch [1/2], Batch 23 Completed\n",
      "images.size(0) :  16\n",
      "Batch 24 Loss: 3852.771191913077\n",
      "Batch 24 Samples: 384\n",
      "Epoch [1/2], Batch 24 Completed\n",
      "images.size(0) :  16\n",
      "Batch 25 Loss: 4003.587568028478\n",
      "Batch 25 Samples: 400\n",
      "Epoch [1/2], Batch 25 Completed\n",
      "images.size(0) :  16\n",
      "Batch 26 Loss: 4159.886444631026\n",
      "Batch 26 Samples: 416\n",
      "Epoch [1/2], Batch 26 Completed\n",
      "images.size(0) :  16\n",
      "Batch 27 Loss: 4312.87189704944\n",
      "Batch 27 Samples: 432\n",
      "Epoch [1/2], Batch 27 Completed\n",
      "images.size(0) :  16\n",
      "Batch 28 Loss: 4470.297927965279\n",
      "Batch 28 Samples: 448\n",
      "Epoch [1/2], Batch 28 Completed\n",
      "images.size(0) :  16\n",
      "Batch 29 Loss: 4630.258263875068\n",
      "Batch 29 Samples: 464\n",
      "Epoch [1/2], Batch 29 Completed\n",
      "images.size(0) :  16\n",
      "Batch 30 Loss: 4782.419616062904\n",
      "Batch 30 Samples: 480\n",
      "Epoch [1/2], Batch 30 Completed\n",
      "images.size(0) :  16\n",
      "Batch 31 Loss: 4937.0092956610715\n",
      "Batch 31 Samples: 496\n",
      "Epoch [1/2], Batch 31 Completed\n",
      "images.size(0) :  16\n",
      "Batch 32 Loss: 5087.452116689398\n",
      "Batch 32 Samples: 512\n",
      "Epoch [1/2], Batch 32 Completed\n",
      "images.size(0) :  16\n",
      "Batch 33 Loss: 5247.408963392285\n",
      "Batch 33 Samples: 528\n",
      "Epoch [1/2], Batch 33 Completed\n",
      "images.size(0) :  16\n",
      "Batch 34 Loss: 5402.946332397405\n",
      "Batch 34 Samples: 544\n",
      "Epoch [1/2], Batch 34 Completed\n",
      "images.size(0) :  16\n",
      "Batch 35 Loss: 5558.091765349542\n",
      "Batch 35 Samples: 560\n",
      "Epoch [1/2], Batch 35 Completed\n",
      "images.size(0) :  16\n",
      "Batch 36 Loss: 5718.112235439401\n",
      "Batch 36 Samples: 576\n",
      "Epoch [1/2], Batch 36 Completed\n",
      "images.size(0) :  16\n",
      "Batch 37 Loss: 5874.72301280517\n",
      "Batch 37 Samples: 592\n",
      "Epoch [1/2], Batch 37 Completed\n",
      "images.size(0) :  16\n",
      "Batch 38 Loss: 6026.937791508094\n",
      "Batch 38 Samples: 608\n",
      "Epoch [1/2], Batch 38 Completed\n",
      "images.size(0) :  16\n",
      "Batch 39 Loss: 6185.177923449027\n",
      "Batch 39 Samples: 624\n",
      "Epoch [1/2], Batch 39 Completed\n",
      "images.size(0) :  16\n",
      "Batch 40 Loss: 6339.502908497407\n",
      "Batch 40 Samples: 640\n",
      "Epoch [1/2], Batch 40 Completed\n",
      "images.size(0) :  16\n",
      "Batch 41 Loss: 6489.682546442982\n",
      "Batch 41 Samples: 656\n",
      "Epoch [1/2], Batch 41 Completed\n",
      "images.size(0) :  16\n",
      "Batch 42 Loss: 6640.91258350956\n",
      "Batch 42 Samples: 672\n",
      "Epoch [1/2], Batch 42 Completed\n",
      "images.size(0) :  16\n",
      "Batch 43 Loss: 6797.3692991953485\n",
      "Batch 43 Samples: 688\n",
      "Epoch [1/2], Batch 43 Completed\n",
      "images.size(0) :  16\n",
      "Batch 44 Loss: 6960.423290623265\n",
      "Batch 44 Samples: 704\n",
      "Epoch [1/2], Batch 44 Completed\n",
      "images.size(0) :  16\n",
      "Batch 45 Loss: 7119.35915753333\n",
      "Batch 45 Samples: 720\n",
      "Epoch [1/2], Batch 45 Completed\n",
      "images.size(0) :  16\n",
      "Batch 46 Loss: 7277.222401210653\n",
      "Batch 46 Samples: 736\n",
      "Epoch [1/2], Batch 46 Completed\n",
      "images.size(0) :  16\n",
      "Batch 47 Loss: 7437.116206036485\n",
      "Batch 47 Samples: 752\n",
      "Epoch [1/2], Batch 47 Completed\n",
      "images.size(0) :  16\n",
      "Batch 48 Loss: 7583.554341679218\n",
      "Batch 48 Samples: 768\n",
      "Epoch [1/2], Batch 48 Completed\n",
      "images.size(0) :  16\n",
      "Batch 49 Loss: 7737.3207722338175\n",
      "Batch 49 Samples: 784\n",
      "Epoch [1/2], Batch 49 Completed\n",
      "images.size(0) :  16\n",
      "Batch 50 Loss: 7898.603524139397\n",
      "Batch 50 Samples: 800\n",
      "Epoch [1/2], Batch 50 Completed\n",
      "images.size(0) :  16\n",
      "Batch 51 Loss: 8066.073470984038\n",
      "Batch 51 Samples: 816\n",
      "Epoch [1/2], Batch 51 Completed\n",
      "images.size(0) :  16\n",
      "Batch 52 Loss: 8218.740750982273\n",
      "Batch 52 Samples: 832\n",
      "Epoch [1/2], Batch 52 Completed\n",
      "images.size(0) :  16\n",
      "Batch 53 Loss: 8371.600264163497\n",
      "Batch 53 Samples: 848\n",
      "Epoch [1/2], Batch 53 Completed\n",
      "images.size(0) :  16\n",
      "Batch 54 Loss: 8522.172241685743\n",
      "Batch 54 Samples: 864\n",
      "Epoch [1/2], Batch 54 Completed\n",
      "images.size(0) :  16\n",
      "Batch 55 Loss: 8675.525572708017\n",
      "Batch 55 Samples: 880\n",
      "Epoch [1/2], Batch 55 Completed\n",
      "images.size(0) :  16\n",
      "Batch 56 Loss: 8827.358002929781\n",
      "Batch 56 Samples: 896\n",
      "Epoch [1/2], Batch 56 Completed\n",
      "images.size(0) :  16\n",
      "Batch 57 Loss: 8978.801440029176\n",
      "Batch 57 Samples: 912\n",
      "Epoch [1/2], Batch 57 Completed\n",
      "images.size(0) :  16\n",
      "Batch 58 Loss: 9132.854408963629\n",
      "Batch 58 Samples: 928\n",
      "Epoch [1/2], Batch 58 Completed\n",
      "images.size(0) :  16\n",
      "Batch 59 Loss: 9277.855726826634\n",
      "Batch 59 Samples: 944\n",
      "Epoch [1/2], Batch 59 Completed\n",
      "images.size(0) :  16\n",
      "Batch 60 Loss: 9425.660113619073\n",
      "Batch 60 Samples: 960\n",
      "Epoch [1/2], Batch 60 Completed\n",
      "images.size(0) :  16\n",
      "Batch 61 Loss: 9583.144190436586\n",
      "Batch 61 Samples: 976\n",
      "Epoch [1/2], Batch 61 Completed\n",
      "images.size(0) :  16\n",
      "Batch 62 Loss: 9737.96249101909\n",
      "Batch 62 Samples: 992\n",
      "Epoch [1/2], Batch 62 Completed\n",
      "images.size(0) :  16\n",
      "Batch 63 Loss: 9897.983740403588\n",
      "Batch 63 Samples: 1008\n",
      "Epoch [1/2], Batch 63 Completed\n",
      "images.size(0) :  16\n",
      "Batch 64 Loss: 10055.88534376835\n",
      "Batch 64 Samples: 1024\n",
      "Epoch [1/2], Batch 64 Completed\n",
      "images.size(0) :  16\n",
      "Batch 65 Loss: 10208.15481912967\n",
      "Batch 65 Samples: 1040\n",
      "Epoch [1/2], Batch 65 Completed\n",
      "images.size(0) :  16\n",
      "Batch 66 Loss: 10359.252140782275\n",
      "Batch 66 Samples: 1056\n",
      "Epoch [1/2], Batch 66 Completed\n",
      "images.size(0) :  16\n",
      "Batch 67 Loss: 10513.417629742335\n",
      "Batch 67 Samples: 1072\n",
      "Epoch [1/2], Batch 67 Completed\n",
      "images.size(0) :  16\n",
      "Batch 68 Loss: 10665.650453201819\n",
      "Batch 68 Samples: 1088\n",
      "Epoch [1/2], Batch 68 Completed\n",
      "images.size(0) :  16\n",
      "Batch 69 Loss: 10810.461280997688\n",
      "Batch 69 Samples: 1104\n",
      "Epoch [1/2], Batch 69 Completed\n",
      "images.size(0) :  16\n",
      "Batch 70 Loss: 10965.554014353154\n",
      "Batch 70 Samples: 1120\n",
      "Epoch [1/2], Batch 70 Completed\n",
      "images.size(0) :  16\n",
      "Batch 71 Loss: 11117.933443463608\n",
      "Batch 71 Samples: 1136\n",
      "Epoch [1/2], Batch 71 Completed\n",
      "images.size(0) :  16\n",
      "Batch 72 Loss: 11268.159018795412\n",
      "Batch 72 Samples: 1152\n",
      "Epoch [1/2], Batch 72 Completed\n",
      "images.size(0) :  16\n",
      "Batch 73 Loss: 11423.127601672515\n",
      "Batch 73 Samples: 1168\n",
      "Epoch [1/2], Batch 73 Completed\n",
      "images.size(0) :  16\n",
      "Batch 74 Loss: 11563.420944874171\n",
      "Batch 74 Samples: 1184\n",
      "Epoch [1/2], Batch 74 Completed\n",
      "images.size(0) :  16\n",
      "Batch 75 Loss: 11716.499377794604\n",
      "Batch 75 Samples: 1200\n",
      "Epoch [1/2], Batch 75 Completed\n",
      "images.size(0) :  16\n",
      "Batch 76 Loss: 11877.611179215348\n",
      "Batch 76 Samples: 1216\n",
      "Epoch [1/2], Batch 76 Completed\n",
      "images.size(0) :  16\n",
      "Batch 77 Loss: 12034.95406254572\n",
      "Batch 77 Samples: 1232\n",
      "Epoch [1/2], Batch 77 Completed\n",
      "images.size(0) :  16\n",
      "Batch 78 Loss: 12185.424430018553\n",
      "Batch 78 Samples: 1248\n",
      "Epoch [1/2], Batch 78 Completed\n",
      "images.size(0) :  16\n",
      "Batch 79 Loss: 12343.116869521467\n",
      "Batch 79 Samples: 1264\n",
      "Epoch [1/2], Batch 79 Completed\n",
      "images.size(0) :  16\n",
      "Batch 80 Loss: 12492.355890294924\n",
      "Batch 80 Samples: 1280\n",
      "Epoch [1/2], Batch 80 Completed\n",
      "images.size(0) :  16\n",
      "Batch 81 Loss: 12650.055439205402\n",
      "Batch 81 Samples: 1296\n",
      "Epoch [1/2], Batch 81 Completed\n",
      "images.size(0) :  16\n",
      "Batch 82 Loss: 12799.875982213693\n",
      "Batch 82 Samples: 1312\n",
      "Epoch [1/2], Batch 82 Completed\n",
      "images.size(0) :  16\n",
      "Batch 83 Loss: 12953.924909819156\n",
      "Batch 83 Samples: 1328\n",
      "Epoch [1/2], Batch 83 Completed\n",
      "images.size(0) :  16\n",
      "Batch 84 Loss: 13106.848578569194\n",
      "Batch 84 Samples: 1344\n",
      "Epoch [1/2], Batch 84 Completed\n",
      "images.size(0) :  16\n",
      "Batch 85 Loss: 13260.968676650147\n",
      "Batch 85 Samples: 1360\n",
      "Epoch [1/2], Batch 85 Completed\n",
      "images.size(0) :  16\n",
      "Batch 86 Loss: 13417.908716030035\n",
      "Batch 86 Samples: 1376\n",
      "Epoch [1/2], Batch 86 Completed\n",
      "images.size(0) :  16\n",
      "Batch 87 Loss: 13571.302987024192\n",
      "Batch 87 Samples: 1392\n",
      "Epoch [1/2], Batch 87 Completed\n",
      "images.size(0) :  16\n",
      "Batch 88 Loss: 13724.017876530837\n",
      "Batch 88 Samples: 1408\n",
      "Epoch [1/2], Batch 88 Completed\n",
      "images.size(0) :  16\n",
      "Batch 89 Loss: 13879.55451233519\n",
      "Batch 89 Samples: 1424\n",
      "Epoch [1/2], Batch 89 Completed\n",
      "images.size(0) :  16\n",
      "Batch 90 Loss: 14026.253811189721\n",
      "Batch 90 Samples: 1440\n",
      "Epoch [1/2], Batch 90 Completed\n",
      "images.size(0) :  16\n",
      "Batch 91 Loss: 14171.819112300576\n",
      "Batch 91 Samples: 1456\n",
      "Epoch [1/2], Batch 91 Completed\n",
      "images.size(0) :  16\n",
      "Batch 92 Loss: 14318.536503184228\n",
      "Batch 92 Samples: 1472\n",
      "Epoch [1/2], Batch 92 Completed\n",
      "images.size(0) :  16\n",
      "Batch 93 Loss: 14471.853549813326\n",
      "Batch 93 Samples: 1488\n",
      "Epoch [1/2], Batch 93 Completed\n",
      "images.size(0) :  16\n",
      "Batch 94 Loss: 14620.094168118798\n",
      "Batch 94 Samples: 1504\n",
      "Epoch [1/2], Batch 94 Completed\n",
      "images.size(0) :  16\n",
      "Batch 95 Loss: 14780.507464230646\n",
      "Batch 95 Samples: 1520\n",
      "Epoch [1/2], Batch 95 Completed\n",
      "images.size(0) :  16\n",
      "Batch 96 Loss: 14932.814178941293\n",
      "Batch 96 Samples: 1536\n",
      "Epoch [1/2], Batch 96 Completed\n",
      "images.size(0) :  16\n",
      "Batch 97 Loss: 15079.487055977723\n",
      "Batch 97 Samples: 1552\n",
      "Epoch [1/2], Batch 97 Completed\n",
      "images.size(0) :  16\n",
      "Batch 98 Loss: 15227.667402497516\n",
      "Batch 98 Samples: 1568\n",
      "Epoch [1/2], Batch 98 Completed\n",
      "images.size(0) :  16\n",
      "Batch 99 Loss: 15383.776544759061\n",
      "Batch 99 Samples: 1584\n",
      "Epoch [1/2], Batch 99 Completed\n",
      "images.size(0) :  16\n",
      "Batch 100 Loss: 15545.155748398978\n",
      "Batch 100 Samples: 1600\n",
      "Epoch [1/2], Batch 100 Completed\n",
      "images.size(0) :  16\n",
      "Batch 101 Loss: 15694.753537139759\n",
      "Batch 101 Samples: 1616\n",
      "Epoch [1/2], Batch 101 Completed\n",
      "images.size(0) :  16\n",
      "Batch 102 Loss: 15845.875833107746\n",
      "Batch 102 Samples: 1632\n",
      "Epoch [1/2], Batch 102 Completed\n",
      "images.size(0) :  16\n",
      "Batch 103 Loss: 16002.479372234064\n",
      "Batch 103 Samples: 1648\n",
      "Epoch [1/2], Batch 103 Completed\n",
      "images.size(0) :  16\n",
      "Batch 104 Loss: 16157.496316327082\n",
      "Batch 104 Samples: 1664\n",
      "Epoch [1/2], Batch 104 Completed\n",
      "images.size(0) :  16\n",
      "Batch 105 Loss: 16314.080685579103\n",
      "Batch 105 Samples: 1680\n",
      "Epoch [1/2], Batch 105 Completed\n",
      "images.size(0) :  16\n",
      "Batch 106 Loss: 16471.13050241388\n",
      "Batch 106 Samples: 1696\n",
      "Epoch [1/2], Batch 106 Completed\n",
      "images.size(0) :  16\n",
      "Batch 107 Loss: 16627.219284069022\n",
      "Batch 107 Samples: 1712\n",
      "Epoch [1/2], Batch 107 Completed\n",
      "images.size(0) :  16\n",
      "Batch 108 Loss: 16780.25298116264\n",
      "Batch 108 Samples: 1728\n",
      "Epoch [1/2], Batch 108 Completed\n",
      "images.size(0) :  16\n",
      "Batch 109 Loss: 16935.46011165977\n",
      "Batch 109 Samples: 1744\n",
      "Epoch [1/2], Batch 109 Completed\n",
      "images.size(0) :  16\n",
      "Batch 110 Loss: 17087.924276026333\n",
      "Batch 110 Samples: 1760\n",
      "Epoch [1/2], Batch 110 Completed\n",
      "images.size(0) :  16\n",
      "Batch 111 Loss: 17238.79458267225\n",
      "Batch 111 Samples: 1776\n",
      "Epoch [1/2], Batch 111 Completed\n",
      "images.size(0) :  16\n",
      "Batch 112 Loss: 17391.290319262982\n",
      "Batch 112 Samples: 1792\n",
      "Epoch [1/2], Batch 112 Completed\n",
      "images.size(0) :  16\n",
      "Batch 113 Loss: 17550.156781207952\n",
      "Batch 113 Samples: 1808\n",
      "Epoch [1/2], Batch 113 Completed\n",
      "images.size(0) :  16\n",
      "Batch 114 Loss: 17707.71746486786\n",
      "Batch 114 Samples: 1824\n",
      "Epoch [1/2], Batch 114 Completed\n",
      "images.size(0) :  16\n",
      "Batch 115 Loss: 17854.847143183528\n",
      "Batch 115 Samples: 1840\n",
      "Epoch [1/2], Batch 115 Completed\n",
      "images.size(0) :  16\n",
      "Batch 116 Loss: 18016.227139462477\n",
      "Batch 116 Samples: 1856\n",
      "Epoch [1/2], Batch 116 Completed\n",
      "images.size(0) :  16\n",
      "Batch 117 Loss: 18166.5739035401\n",
      "Batch 117 Samples: 1872\n",
      "Epoch [1/2], Batch 117 Completed\n",
      "images.size(0) :  16\n",
      "Batch 118 Loss: 18316.27791075054\n",
      "Batch 118 Samples: 1888\n",
      "Epoch [1/2], Batch 118 Completed\n",
      "images.size(0) :  16\n",
      "Batch 119 Loss: 18475.088471285246\n",
      "Batch 119 Samples: 1904\n",
      "Epoch [1/2], Batch 119 Completed\n",
      "images.size(0) :  16\n",
      "Batch 120 Loss: 18630.033100302444\n",
      "Batch 120 Samples: 1920\n",
      "Epoch [1/2], Batch 120 Completed\n",
      "images.size(0) :  16\n",
      "Batch 121 Loss: 18783.41047262831\n",
      "Batch 121 Samples: 1936\n",
      "Epoch [1/2], Batch 121 Completed\n",
      "images.size(0) :  16\n",
      "Batch 122 Loss: 18938.595083291937\n",
      "Batch 122 Samples: 1952\n",
      "Epoch [1/2], Batch 122 Completed\n",
      "images.size(0) :  16\n",
      "Batch 123 Loss: 19090.697716463263\n",
      "Batch 123 Samples: 1968\n",
      "Epoch [1/2], Batch 123 Completed\n",
      "images.size(0) :  16\n",
      "Batch 124 Loss: 19250.193537202318\n",
      "Batch 124 Samples: 1984\n",
      "Epoch [1/2], Batch 124 Completed\n",
      "images.size(0) :  16\n",
      "Batch 125 Loss: 19411.874044315242\n",
      "Batch 125 Samples: 2000\n",
      "Epoch [1/2], Batch 125 Completed\n",
      "images.size(0) :  16\n",
      "Batch 126 Loss: 19565.13592162359\n",
      "Batch 126 Samples: 2016\n",
      "Epoch [1/2], Batch 126 Completed\n",
      "images.size(0) :  16\n",
      "Batch 127 Loss: 19726.231223609255\n",
      "Batch 127 Samples: 2032\n",
      "Epoch [1/2], Batch 127 Completed\n",
      "images.size(0) :  16\n",
      "Batch 128 Loss: 19881.38651832076\n",
      "Batch 128 Samples: 2048\n",
      "Epoch [1/2], Batch 128 Completed\n",
      "images.size(0) :  16\n",
      "Batch 129 Loss: 20033.784918725945\n",
      "Batch 129 Samples: 2064\n",
      "Epoch [1/2], Batch 129 Completed\n",
      "images.size(0) :  16\n",
      "Batch 130 Loss: 20181.3108214693\n",
      "Batch 130 Samples: 2080\n",
      "Epoch [1/2], Batch 130 Completed\n",
      "images.size(0) :  16\n",
      "Batch 131 Loss: 20335.89866956136\n",
      "Batch 131 Samples: 2096\n",
      "Epoch [1/2], Batch 131 Completed\n",
      "images.size(0) :  16\n",
      "Batch 132 Loss: 20486.383348584855\n",
      "Batch 132 Samples: 2112\n",
      "Epoch [1/2], Batch 132 Completed\n",
      "images.size(0) :  16\n",
      "Batch 133 Loss: 20635.790641600714\n",
      "Batch 133 Samples: 2128\n",
      "Epoch [1/2], Batch 133 Completed\n",
      "images.size(0) :  16\n",
      "Batch 134 Loss: 20791.11881473971\n",
      "Batch 134 Samples: 2144\n",
      "Epoch [1/2], Batch 134 Completed\n",
      "images.size(0) :  16\n",
      "Batch 135 Loss: 20953.166055924892\n",
      "Batch 135 Samples: 2160\n",
      "Epoch [1/2], Batch 135 Completed\n",
      "images.size(0) :  16\n",
      "Batch 136 Loss: 21114.25861385868\n",
      "Batch 136 Samples: 2176\n",
      "Epoch [1/2], Batch 136 Completed\n",
      "images.size(0) :  16\n",
      "Batch 137 Loss: 21270.817171703326\n",
      "Batch 137 Samples: 2192\n",
      "Epoch [1/2], Batch 137 Completed\n",
      "images.size(0) :  16\n",
      "Batch 138 Loss: 21426.411337481775\n",
      "Batch 138 Samples: 2208\n",
      "Epoch [1/2], Batch 138 Completed\n",
      "images.size(0) :  16\n",
      "Batch 139 Loss: 21581.72311620002\n",
      "Batch 139 Samples: 2224\n",
      "Epoch [1/2], Batch 139 Completed\n",
      "images.size(0) :  16\n",
      "Batch 140 Loss: 21731.45974466576\n",
      "Batch 140 Samples: 2240\n",
      "Epoch [1/2], Batch 140 Completed\n",
      "images.size(0) :  16\n",
      "Batch 141 Loss: 21887.73812666555\n",
      "Batch 141 Samples: 2256\n",
      "Epoch [1/2], Batch 141 Completed\n",
      "images.size(0) :  16\n",
      "Batch 142 Loss: 22047.967312608384\n",
      "Batch 142 Samples: 2272\n",
      "Epoch [1/2], Batch 142 Completed\n",
      "images.size(0) :  16\n",
      "Batch 143 Loss: 22194.658948786313\n",
      "Batch 143 Samples: 2288\n",
      "Epoch [1/2], Batch 143 Completed\n",
      "images.size(0) :  16\n",
      "Batch 144 Loss: 22349.451283628645\n",
      "Batch 144 Samples: 2304\n",
      "Epoch [1/2], Batch 144 Completed\n",
      "images.size(0) :  16\n",
      "Batch 145 Loss: 22503.570787538087\n",
      "Batch 145 Samples: 2320\n",
      "Epoch [1/2], Batch 145 Completed\n",
      "images.size(0) :  16\n",
      "Batch 146 Loss: 22652.42934935192\n",
      "Batch 146 Samples: 2336\n",
      "Epoch [1/2], Batch 146 Completed\n",
      "images.size(0) :  16\n",
      "Batch 147 Loss: 22796.570434761652\n",
      "Batch 147 Samples: 2352\n",
      "Epoch [1/2], Batch 147 Completed\n",
      "images.size(0) :  16\n",
      "Batch 148 Loss: 22953.297802880308\n",
      "Batch 148 Samples: 2368\n",
      "Epoch [1/2], Batch 148 Completed\n",
      "images.size(0) :  16\n",
      "Batch 149 Loss: 23108.17459065895\n",
      "Batch 149 Samples: 2384\n",
      "Epoch [1/2], Batch 149 Completed\n",
      "images.size(0) :  16\n",
      "Batch 150 Loss: 23270.587009187293\n",
      "Batch 150 Samples: 2400\n",
      "Epoch [1/2], Batch 150 Completed\n",
      "images.size(0) :  16\n",
      "Batch 151 Loss: 23428.791888662727\n",
      "Batch 151 Samples: 2416\n",
      "Epoch [1/2], Batch 151 Completed\n",
      "images.size(0) :  16\n",
      "Batch 152 Loss: 23584.642957709595\n",
      "Batch 152 Samples: 2432\n",
      "Epoch [1/2], Batch 152 Completed\n",
      "images.size(0) :  16\n",
      "Batch 153 Loss: 23737.06048469453\n",
      "Batch 153 Samples: 2448\n",
      "Epoch [1/2], Batch 153 Completed\n",
      "images.size(0) :  16\n",
      "Batch 154 Loss: 23893.00971126989\n",
      "Batch 154 Samples: 2464\n",
      "Epoch [1/2], Batch 154 Completed\n",
      "Epoch [1/2], Loss: 9.6968\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# chunk 0-10 for gun0-gun6\n",
    "PatchList = ['gun0_chunk_0.parquet', 'gun0_chunk_1.parquet', 'gun0_chunk_2.parquet', 'gun0_chunk_3.parquet', 'gun0_chunk_4.parquet', 'gun0_chunk_5.parquet', 'gun0_chunk_6.parquet', 'gun0_chunk_7.parquet', 'gun0_chunk_8.parquet', 'gun0_chunk_9.parquet', 'gun0_chunk_10.parquet']\n",
    "model = TrainBlock(PatchList, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 9.8101\n"
     ]
    }
   ],
   "source": [
    "TestList = ['test0.parquet', 'test1.parquet', 'test2.parquet']\n",
    "model = TestBlock(TestList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to disk\n",
    "torch.save(model.state_dict(), 'RegressionModel.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
